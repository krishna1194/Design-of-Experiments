{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9ee1ec",
   "metadata": {},
   "source": [
    "# Design of Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce7e44",
   "metadata": {},
   "source": [
    "There are two main types of DOE:\n",
    "- Qasi Experimental design       : **NO Randomized Assignment** of Test and Control group\n",
    "- Non-Qasi Experimental design   : **Randomized Assignment** of Test and Control group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf9dc2d",
   "metadata": {},
   "source": [
    "## Basics of A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb48d047",
   "metadata": {},
   "source": [
    "A/B testing falls under Non-Qasi Experimental design\n",
    "\n",
    "\n",
    "* What is an A/B Test?\n",
    "    - A/B testing is a process of randomly assigning people into two (or more) groups to observe and select the version that drives most business impact for the organization.\n",
    "\n",
    "\n",
    "* Though there are tools available, what is the disadvantage of using them?\n",
    "    - Integration issue: If webpage is tagged with one vendor but the A/B solution is obtained from different vendor, then there are complications while integrating them.\n",
    "    - Latency issue: There might be some kind of slowness in page or flickering, because a javascript is needed at both client and server ends to track the data that leads to this issue.\n",
    "    - Limited ebility to test the changes: Stakeholders are restricted to make changes that are provided by the tool and request for something specific.\n",
    "    - Privacy issue: Due to GDPR and other laws, it's not easy to trasfer all the data into the tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d711e61",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e69d63",
   "metadata": {},
   "source": [
    "We won't be using the whole population, the conversion rates that we'll get will inevitably be only estimates of the true rates.\n",
    "\n",
    "The number of people (or user sessions) we decide to capture in each group will have an effect on the precision of our estimated conversion rates: **the larger the sample size**, the more precise our estimates (i.e. the smaller our confidence intervals), **the higher the chance to detect a difference in the two groups, if present**. On the other hand, the larger our sample gets, the more expensive (and impractical) our study becomes.\n",
    "\n",
    "The sample size we need is estimated through something called Power analysis(The **main purpose underlying power analysis is to determine the smallest sample size that is suitable to detect the effect of a given test at the desired level of significance**), and it depends on a few factors:\n",
    "\n",
    "- **Power of the test (1-Œ≤)** - This represents the probability of finding a statistical difference between the groups in our test when a difference is actually present. This is usually set at 0.8 as a convention\n",
    "- **Significance Level: Alpha value (Œ±)** - The critical value we set to 0.05\n",
    "- **Effect size** - How big of a difference we expect there to be between the conversion rates\n",
    "\n",
    "Since we need a difference of 2%, we can use 13% and 15% to calculate the effect size.\n",
    "\n",
    "In Python: https://www.statsmodels.org/dev/generated/statsmodels.stats.power.NormalIndPower.solve_power.html\n",
    "\n",
    "By Hand: \n",
    "\n",
    "**n = (16*œÉ¬≤) / ùõø¬≤**\n",
    "\n",
    "- n: number of samples.\n",
    "- œÉ¬≤: sample variance.\n",
    "- ùõø: the difference between the treatment and control groups (in percentage).\n",
    "\n",
    "\n",
    "\n",
    "***For Understanding:***\n",
    "\n",
    "Here is the bi-relationship between these three parameters and the required sample size:\n",
    "\n",
    "- Significance Level decreases ‚Üí Larger Sample Size\n",
    "- Statistical Power increases ‚Üí Larger Sample Size\n",
    "- The Minimum Detectable Effect decreases ‚Üí Larger Sample Size\n",
    "\n",
    "\n",
    "- Œ≤ = probability of a Type II error, known as a \"false negative\"\n",
    "- 1 ‚àí Œ≤ = probability of a \"true positive\", i.e., correctly rejecting the null hypothesis. \"1 ‚àí Œ≤\" is also known as the power of the test.\n",
    "- Œ± = probability of a Type I error, known as a \"false positive\"\n",
    "- 1 ‚àí Œ± = probability of a \"true negative\", i.e., correctly not rejecting the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ff0f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import statsmodels.stats.api as sms\n",
    "from math import ceil\n",
    "\n",
    "# Caculate the Sample Size\n",
    "required_n = sms.NormalIndPower().solve_power(\n",
    "    0.1, \n",
    "    power=0.8, \n",
    "    alpha=0.05,\n",
    "    nobs1= None,\n",
    "    ratio=1\n",
    "    )                                                  # Calculating sample size needed\n",
    "\n",
    "required_n = ceil(required_n)                          # Rounding up to next whole number                          \n",
    "\n",
    "print(required_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac703f22",
   "metadata": {},
   "source": [
    "So, there is a need of 1570 users for each group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b07a0d",
   "metadata": {},
   "source": [
    "## Randomization in A/B Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b331b",
   "metadata": {},
   "source": [
    "* What is an Randomization in A/B Tests?\n",
    "    - It is the process by which we allocate users to either Treatment or Control group in a A/B test. We have to make sure the users are randomly assigned and there should not be any information conveyed by the way we assign the users. (Remove the nuances factors that might give information about the groups.) One specific user should have visited only either Control or Treatment but not both.\n",
    "    \n",
    "    \n",
    "* What is **SUTVA (Stable Unit Treatment Value Assumption)**?\n",
    "    - SUTVA states that the treatment and control units don‚Äôt interact with each other and are independent of each other; otherwise, the interference leads to biased estimates.\n",
    "    \n",
    "\n",
    "* When SUTVA is violated?\n",
    "    - A users experience both control and treatment variations in a single session.\n",
    "    - Resources are shared among experiment subjects. Ex: two people using the same Email Id to operate a youtube premium account and the unit of randomization for the experiment being Login ID\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a06e4c",
   "metadata": {},
   "source": [
    "## SRM: Sample Ratio Mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e8f2e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* What is SRM?\n",
    "    - SRM is Sample Ratio Mismatch. In simple words after spliting population into groups(traffic allocation) and introduce a variation(treatment), we expect the traffic to flow equally into two groups but it doesn't happen. One group receives many more visitors(page views or sessions) than the other, the ratio of traffic is not equal. And you have a Sample Ratio Mismatch issue.\n",
    "    \n",
    "* Why SRM an issue?\n",
    "    - When SRM occurs the results can't be trusted because traffic skews conversion numbers. \n",
    "    \n",
    "    To understand the concept much better let's look at an example: If we run a A/B test with equal split and leave the test for 5 weeks, and get a total of 579,286 total visitors, then the expection in control and treatment group would be 289,643 visitors each. But, let's say this is not the case. The Control group has 247,563 visitors and Treatment group has 331,723 visitors. Let's say for understanding purpose, we are looking to increase conversions, the control achieved 4,735 conversions, but the test slightly outperformed with 5,323 conversions.\n",
    "    \n",
    "    Now the question is: Is this going to affect the final output? Ans is **YES**\n",
    "    \n",
    "    Consider Conversion Rate **with and without SRM issue**: \n",
    "        \n",
    "        - Calcultion for Conversion Rate = Conversions/Visitors. \n",
    "    \n",
    "        - Calculation for Effect = (Conversion Rate of Treatment - Conversion Rate of Control) / (Conversion Rate of Control)\n",
    "    \n",
    "    |SRM Issue?|Metric|Contol|Treatment|Effect|\n",
    "    |-----|-----|-----|-----|-----|\n",
    "    |N|Visitors|289,643|289,643||\n",
    "    |N|Conversions|4,735|5,323||\n",
    "    |N|Conversion Rate|1.63%|**1.84%**|**12.88%**|\n",
    "    ||||||\n",
    "    ||||||\n",
    "    |Y|Visitors|247,563|331,723||\n",
    "    |Y|Conversions|4,735|5,323||\n",
    "    |Y|Conversion Rate|**1.91%**|1.84%|**-16.23%**|\n",
    "    \n",
    "    From the above table we understand that SRM keeps our real result in the dark. The data is inaccurate. So, we are going to avoid SRM. SRM a gold standard of reliable test results; by checking for SRM, and verifying your test doesn't have an SRM issue, you can be more confident results are trustworthy.\n",
    "    \n",
    "    Let's try to perform this on Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99754b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.339953870327957\n"
     ]
    }
   ],
   "source": [
    "# Sample Ratio Calculation\n",
    "control_visitors = 247563\n",
    "treatment_visitors = 331723\n",
    "\n",
    "sample_ratio = treatment_visitors / control_visitors\n",
    "print(sample_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056cf647",
   "metadata": {},
   "source": [
    "The sample ratio is 1.34 while the design ratio for which we had planned, was 1. In order to determine if this occurrence is statistically significant or just by chance, we need to calculate the p-value of the Sample Ratio.\n",
    "\n",
    "\n",
    "* How do you identify SRM in A/B test:\n",
    "    To calculate the p-value of Sample Ratio we perform either **standard T-test or a chi-squared test** \n",
    "\n",
    "    **Null Hypothesis:** There is no difference between the treatment group and the control group\n",
    "\n",
    "    **Alternate Hypothesis:** There is a significant difference between the two groups.\n",
    "    \n",
    "    If p-value <= 0.01, we are essentially saying that there is only a 1% chance or less of obtaining a false positive a.k.a there is only a 1% chance we say that there is a difference between the two groups when in reality there isn't any. For a p-value <= 0.01, we reject our null hypothesis and conclude that the two groups are different (which is what we are checking for when we observe a different sample ratio when compared to the design ratio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5342f9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuction to compute the p values for an A/B Test.\n",
    "from scipy.stats import chisquare\n",
    "from typing import Union\n",
    "\n",
    "def chi_stat_sig(treatment:Union[int,float], control:Union[int,float], alpha:float) -> str:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ---------------\n",
    "    treatement : the count of the unit of randomization in the treatement group (can be a cookie/user id/ device id)\n",
    "    control : the count of the unit of randomization in the control group (can be a cookie/user id/ device id)\n",
    "    alpha : the statistical significance boundry\n",
    "    \"\"\"\n",
    "    if not isinstance(alpha,float):\n",
    "        raise TypeError(\"variable alpha is not of type float\")\n",
    "        \n",
    "    control_visitors = control\n",
    "    treatment_visitors = treatment\n",
    "    observed_visitors = [treatment_visitors, control_visitors]\n",
    "    total_visitors = treatment_visitors + control_visitors\n",
    "    expected_visitors = [total_visitors/2, total_visitors/2]\n",
    "    \n",
    "    chi = chisquare(observed_visitors,f_exp=expected_visitors)\n",
    "    if chi[1] <= alpha:\n",
    "        return(f'the p-value for the chi squared test is {chi[1]} and there is a difference between treatment group and control group')\n",
    "    else:\n",
    "        return(f'the p-value for the chi squared test is {chi[1]} and there is \"NO\" difference between treatment group and control group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a769d61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the p-value for the chi squared test is 0.0 and there is a difference between treatment group and control group'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_stat_sig(331723,247563,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e31cd",
   "metadata": {},
   "source": [
    "The calculated p-value is less than 0.01 for our Sample Ratio and hence we will reject the Null hypothesis. It is very likely that our implementation of the A/B test is incorrect and that is what has caused a Sample Ratio Mismatch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a4e48",
   "metadata": {},
   "source": [
    "* Why SRM happens?\n",
    "    - Test not being set-up properly or not randomized properly\n",
    "    - Bug in test or test set-up (testing variants where the users are redirected to a new experience - Browser Redirects)\n",
    "    - Tracking or reporting issues\n",
    "    - Traffic from Bots\n",
    "    \n",
    "    \n",
    "* What to do about SRM issues?\n",
    "    - Review test set-up\n",
    "    - Look for broader trends in test data to understand where problem is present\n",
    "    - Re-run the study\n",
    "    - Run an A/A test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
